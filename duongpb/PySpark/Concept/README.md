# Definition
Apache Spark is a unified analytics engine for large-scale data processing.
It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. 
It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.


We have 2 type of spark which are : 
- spark RDD 
- spark Dataframe
- spark Dataset

Spark can run in three different modes: Standalone, YARN and Mesos

Spark can also be use as an alternative of MapReduce in Hadoop eco system so 
in order to run a spark pineline, it's recommended that we use hadoop eco system
to run it instead of standalone mode.

so in order to install spark, we need to install these 4 things: 
1. java
2. python
3. spark
4. hadoop